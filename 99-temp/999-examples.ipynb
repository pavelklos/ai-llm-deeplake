{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b5c45e9-0e1c-4f65-96da-755422e59c2a",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48bcfc-d78f-43e2-891c-237d902027a1",
   "metadata": {},
   "source": [
    "### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6a91e47-a279-4560-a2fc-cc5c57b9157c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavel\\miniconda3\\envs\\llms\\Lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (4.1.14) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import deeplake\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables \n",
    "load_dotenv(override = True)\n",
    "open_api_key = os.getenv('OPENAI_API_KEY')\n",
    "activeloop_token = os.getenv('ACTIVELOOP_TOKEN')\n",
    "\n",
    "# Set environment variables\n",
    "# os.environ['OPENAI_API_KEY'] = open_api_key\n",
    "# os.environ['ACTIVELOOP_TOKEN'] = activeloop_token\n",
    "\n",
    "# Set user-defined constants\n",
    "DATASET_PATH_1 = \"hub://pavelkloscz/twitter_algorithm_twml_2\"\n",
    "DATASET_PATH_2 = \"hub://pavelkloscz/twitter_algorithm\"\n",
    "DATASET_PATH_3 = \"hub://pavelkloscz/val2017-100\"\n",
    "DATASET_PATH_4 = \"hub://pavelkloscz/ds-scifact\"\n",
    "LOCAL_PATH_1 = \"./twitter_algorithm_twml_2\"  # 361 kB (40 files, 25 directories)\n",
    "LOCAL_PATH_2 = \"./twitter_algorithm\"  # 147 kB (40 files, 25 directories)\n",
    "LOCAL_PATH_3 = \"./val2017-100\"  # 15,1 MB (71 files, 50 directories)\n",
    "LOCAL_PATH_4 = \"./ds-scifact\"  # 38,1 MB (40 files, 25 directories)\n",
    "\n",
    "# NIH Chest X-rays (Over 112,000 Chest X-ray images from more than 30,000 unique patients)**<br>\n",
    "# [NIH Chest X-Ray (DeepLake)]\n",
    "# - https://app.activeloop.ai/activeloop/nih-chest-xray-train)\n",
    "# [NIH Chest X-Ray (Kaggle)]\n",
    "# - https://www.kaggle.com/datasets/nih-chest-xrays/data\n",
    "DATASET_PATH_5 = \"hub://activeloop/nih-chest-xray-train\"\n",
    "LOCAL_PATH_5 = \"./nih-chest-xray-train\"  # 38,1 MB (40 files, 25 directories)\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e4c910-b60d-4a79-a6dc-432eb43512eb",
   "metadata": {},
   "source": [
    "### Download dataset from DeepLake hub and save it locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d33fde1-281e-4dc1-9860-b66042ccbbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./twitter_algorithm_twml_2 loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating copy transform: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:04<00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./twitter_algorithm_twml_2 loaded successfully.\n",
      "\n",
      "Dataset downloaded from hub://pavelkloscz/twitter_algorithm_twml_2 and saved to ./twitter_algorithm_twml_2\n",
      "Dataset(path='./twitter_algorithm_twml_2', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      " embedding  embedding  (24, 1536)  float32   None   \n",
      "    id        text      (24, 1)      str     None   \n",
      " metadata     json      (24, 1)      str     None   \n",
      "   text       text      (24, 1)      str     None   \n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# URL of the dataset in DeepLake hub\n",
    "dataset_path = DATASET_PATH_1\n",
    "\n",
    "# Local path where you want to save the dataset\n",
    "local_path = LOCAL_PATH_1\n",
    "\n",
    "# Download the dataset from hub to local storage\n",
    "deeplake.copy(\n",
    "    src=dataset_path,\n",
    "    dest=local_path,\n",
    "    overwrite=True  # Set to False if you want to avoid overwriting existing data\n",
    ")\n",
    "\n",
    "# Load the local dataset to verify\n",
    "local_dataset = deeplake.load(local_path)\n",
    "print(f\"Dataset downloaded from {dataset_path} and saved to {local_path}\")\n",
    "print(local_dataset.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7fa8ae7-7525-43c3-ad0b-9ed3372bf3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access specific tensors\n",
    "texts = local_dataset.text.numpy()\n",
    "embeddings = local_dataset.embedding.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec91f599-bedd-42b4-9527-bc45653106b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))\n",
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcba9653-7c76-4c97-8452-3cce2636a0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# pylint: disable=wildcard-import\\r\\n\"\"\"\\r\\nThis module contains the ``tf.layers.Layer`` subclasses implemented in twml.\\r\\nLayers are used to instantiate common subgraphs.\\r\\nTypically, these layers are used when defining a ``build_graph_fn``\\r\\nfor the ``twml.trainers.Trainer``.\\r\\n\"\"\"\\r\\n\\r\\nfrom .batch_prediction_tensor_writer import BatchPredictionTensorWriter  # noqa: F401\\r\\nfrom .batch_prediction_writer import BatchPredictionWriter  # noqa: F401\\r\\nfrom .data_record_tensor_writer import DataRecordTensorWriter  # noqa: F401\\r\\nfrom .full_dense import full_dense, FullDense  # noqa: F401\\r\\nfrom .full_sparse import full_sparse, FullSparse  # noqa: F401\\r\\nfrom .isotonic import Isotonic  # noqa: F401\\r\\nfrom .layer import Layer  # noqa: F401\\r\\nfrom .mdl import MDL  # noqa: F401\\r\\nfrom .partition import Partition  # noqa: F401\\r\\nfrom .percentile_discretizer import PercentileDiscretizer  # noqa: F401\\r\\nfrom .sequential import Sequential  # noqa: F401\\r\\nfrom .sparse_max_norm import MaxNorm, sparse_max_norm, SparseMaxNorm  # noqa: F401\\r\\nfrom .stitch import Stitch  # noqa: F401']\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbd47d08-b011-41a8-96c5-90aa8ff37884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00466384  0.01501529 -0.02373155 ... -0.00217907 -0.01788753\n",
      " -0.03816387]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775aa9f4-0d49-4dbd-89ff-a9003fd2703f",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4ae14df-9e9a-43eb-845b-3be3d8a85ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_dataset(dataset_path, local_path):\n",
    "    # URL of the dataset in DeepLake hub\n",
    "    # dataset_path = DATASET_PATH_1\n",
    "\n",
    "    # Local path where you want to save the dataset\n",
    "    # local_path = LOCAL_PATH_1\n",
    "\n",
    "    # Download the dataset from hub to local storage\n",
    "    deeplake.copy(\n",
    "        src=dataset_path,\n",
    "        dest=local_path,\n",
    "        overwrite=True  # Set to False if you want to avoid overwriting existing data\n",
    "    )\n",
    "\n",
    "    # Load the local dataset to verify\n",
    "    local_dataset = deeplake.load(local_path)\n",
    "    print(f\"Dataset downloaded from {dataset_path} and saved to {local_path}\")\n",
    "    print(local_dataset.summary())\n",
    "\n",
    "    return local_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b7290-a7f0-45c3-bbf3-a3df61e0e7e3",
   "metadata": {},
   "source": [
    "## TRY-CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3878971a-4882-4ba0-9e0d-b8ded6ea0cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./twitter_algorithm loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating copy transform: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:05<00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./twitter_algorithm loaded successfully.\n",
      "\n",
      "Dataset downloaded from hub://pavelkloscz/twitter_algorithm and saved to ./twitter_algorithm\n",
      "Dataset(path='./twitter_algorithm', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      " embedding  embedding  (22, 1536)  float32   None   \n",
      "    id        text      (22, 1)      str     None   \n",
      " metadata     json      (22, 1)      str     None   \n",
      "   text       text      (22, 1)      str     None   \n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_2 = copy_dataset(DATASET_PATH_2, LOCAL_PATH_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a10494ec-0cf7-40a9-94f2-99b9d2886369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./val2017-100 loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating copy transform: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./val2017-100 loaded successfully.\n",
      "\n",
      "Dataset downloaded from hub://pavelkloscz/val2017-100 and saved to ./val2017-100\n",
      "Dataset(path='./val2017-100', tensors=['embedding', 'filename', 'id', 'image'])\n",
      "\n",
      "  tensor      htype               shape               dtype  compression\n",
      "  -------    -------             -------             -------  ------- \n",
      " embedding  embedding           (100, 512)           float32   None   \n",
      " filename     text               (100, 1)              str     None   \n",
      "    id        text               (100, 1)              str     None   \n",
      "   image      image    (100, 240:640, 320:640, 1:3)   uint8    jpeg   \n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "dataset_3 = copy_dataset(DATASET_PATH_3, LOCAL_PATH_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eec91f6e-801a-42b4-b838-2f5c08b475ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ds-scifact loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating copy transform: 100%|███████████████████████████████████████████████████████████████████████████████████████| 5183/5183 [00:23<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ds-scifact loaded successfully.\n",
      "\n",
      "Dataset downloaded from hub://pavelkloscz/ds-scifact and saved to ./ds-scifact\n",
      "Dataset(path='./ds-scifact', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape       dtype  compression\n",
      "  -------    -------     -------     -------  ------- \n",
      " embedding  embedding  (5183, 1536)  float32   None   \n",
      "    id        text      (5183, 1)      str     None   \n",
      " metadata     json      (5183, 1)      str     None   \n",
      "   text       text      (5183, 1)      str     None   \n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "dataset_4 = copy_dataset(DATASET_PATH_4, LOCAL_PATH_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730af985-69ff-481a-aeb7-364ff5029ea2",
   "metadata": {},
   "source": [
    "## TRY-CHECK (macOS) \"NIH Chest X-rays\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfab1bce-9d03-4970-b697-61d4788c92b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./nih-chest-xray-train loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating copy transform: 9%|████████▎                                                                               | 8188/86524 [02:31<24:09"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./nih-chest-xray-train loaded successfully.\n",
      "\n",
      "Dataset downloaded from hub://activeloop/nih-chest-xray-train and saved to ./nih-chest-xray-train\n",
      "Dataset(path='./nih-chest-xray-train', tensors=['findings', 'images', 'metadata/follow_up_num', 'metadata/orig_img_h', 'metadata/orig_img_pix_spacing_x', 'metadata/orig_img_pix_spacing_y', 'metadata/orig_img_w', 'metadata/patient_age', 'metadata/patient_gender', 'metadata/patient_id', 'metadata/view_position'])\n",
      "\n",
      "             tensor                  htype        shape       dtype  compression\n",
      "             -------                -------      -------     -------  ------- \n",
      "            findings              class_label  (86524, 1:9)  uint32    None   \n",
      "             images                  image         (0,)       uint8     png   \n",
      "     metadata/follow_up_num         generic        (0,)       int32    None   \n",
      "       metadata/orig_img_h          generic        (0,)      uint32    None   \n",
      " metadata/orig_img_pix_spacing_x    generic        (0,)      float32   None   \n",
      " metadata/orig_img_pix_spacing_y    generic        (0,)      float32   None   \n",
      "       metadata/orig_img_w          generic        (0,)      uint32    None   \n",
      "      metadata/patient_age          generic        (0,)       int32    None   \n",
      "     metadata/patient_gender      class_label      (0,)      uint32    None   \n",
      "       metadata/patient_id          generic        (0,)       int32    None   \n",
      "     metadata/view_position       class_label      (0,)      uint32    None   \n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_5 = copy_dataset(DATASET_PATH_5, LOCAL_PATH_5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
