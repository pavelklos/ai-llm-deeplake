{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93495a2e-3e04-4cb8-81d7-a00f9dc0b4f4",
   "metadata": {},
   "source": [
    "# EXAMPLES (RAG)\n",
    "- [RAG](https://docs.activeloop.ai/examples/rag)\n",
    "  - [RAG Quickstart](https://docs.activeloop.ai/examples/rag/quickstart)\n",
    "  - [RAG Tutorials](https://docs.activeloop.ai/examples/rag/tutorials)\n",
    "    - [Vector Store Basics](https://docs.activeloop.ai/examples/rag/tutorials/vector-store-basics)\n",
    "    - [Vector Search Options](https://docs.activeloop.ai/examples/rag/tutorials/vector-search-options)\n",
    "      - [LangChain API](https://docs.activeloop.ai/examples/rag/tutorials/vector-search-options/langchain-api)\n",
    "      - [Deep Lake Vector Store API](https://docs.activeloop.ai/examples/rag/tutorials/vector-search-options/vector-store-api)\n",
    "      - [Managed Database REST API](https://docs.activeloop.ai/examples/rag/tutorials/vector-search-options/rest-api)\n",
    "    - [Customizing Your Vector Store](https://docs.activeloop.ai/examples/rag/tutorials/step-4-customizing-vector-stores)\n",
    "    - [Image Similarity Search](https://docs.activeloop.ai/examples/rag/tutorials/image-similarity-search)\n",
    "    - [Improving Search Accuracy using Deep Memory](https://docs.activeloop.ai/examples/rag/tutorials/deepmemory)\n",
    "  - [LangChain Integration](https://docs.activeloop.ai/examples/rag/langchain-integration)\n",
    "  - [**LlamaIndex Integration**](https://docs.activeloop.ai/examples/rag/llamaindex-integration)\n",
    "  - [Managed Tensor Database](https://docs.activeloop.ai/examples/rag/managed-database)\n",
    "    - [REST API](https://docs.activeloop.ai/examples/rag/managed-database/rest-api)\n",
    "    - [Migrating Datasets to the Tensor Database](https://docs.activeloop.ai/examples/rag/managed-database/migrating-datasets-to-the-tensor-database)\n",
    "  - [Deep Memory](https://docs.activeloop.ai/examples/rag/deep-memory)\n",
    "    - [How it Works](https://docs.activeloop.ai/examples/rag/deep-memory/how-it-works)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9134e12-66df-4b48-915d-a3b9f5e1c550",
   "metadata": {},
   "source": [
    "## RAG (LlamaIndex Integration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a96007e-3a02-4498-8127-e30e1447c543",
   "metadata": {},
   "source": [
    "### Use Deep Lake as a Vector Store in LlamaIndex\n",
    "*Deep Lake can be used as a VectorStore in LlamaIndex for building Apps that require filtering and vector search. In this tutorial we will show how to create a Deep Lake Vector Store in LangChain and use it to build a Q&A App about the Twitter OSS recommendation algorithm.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e5667-baa8-403b-b4e0-478f93a741b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip3 install llama-index-vector-stores-deeplake\n",
    "# !pip3 install langchain llama-index deeplake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc4a49f9-28d0-4df6-8275-6016b3911d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-vector-stores-deeplake\n",
    "# !pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a370e3be-21ee-4be5-8057-13eb9514d93f",
   "metadata": {},
   "source": [
    "#### Downloading and Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb062e7-e7da-4b50-aacb-d9669e05e69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavel\\projects\\ai-llm-deeplake\\.venv\\Lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (4.1.14) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import textwrap\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5996ee04-9d9f-46fd-855b-a11deead1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override = True)\n",
    "open_api_key = os.getenv('OPENAI_API_KEY')\n",
    "activeloop_token = os.getenv('ACTIVELOOP_TOKEN')\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af670a7f-5c7c-46a1-b25c-038fd6a2147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the Twitter OSS recommendation algorithm\n",
    "\n",
    "# !git clone https://github.com/twitter/the-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e06eee1b-a33d-4556-b7c6-4f25b8290a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a local path to the files and add a reader for processing and chunking them\n",
    "\n",
    "# repo_path = '/the-algorithm'\n",
    "# repo_path = './the-algorithm'\n",
    "repo_path = './the-algorithm/twml/twml/layers'\n",
    "\n",
    "documents = SimpleDirectoryReader(repo_path, recursive=True).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb47962b-acbd-4df2-96ad-24440755fba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print(type(documents))\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaf5044-b172-4941-b72a-c2c8a330afbf",
   "metadata": {},
   "source": [
    "#### Creating the Deep Lake Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b600c13-ba1c-4eb4-a504-d368c02a8eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "# Create an empty Deep Lake Vector Store using a specified path\n",
    "\n",
    "# dataset_path = 'hub://<org-id>/twitter_algorithm'\n",
    "dataset_path = 'hub://pavelkloscz/twitter_algorithm_twml_2'  # [twml] subdirectory of this github repo\n",
    "vector_store = DeepLakeVectorStore(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe8afd36-f4e0-4aa2-a7e4-5e1b80168a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Lake Vector Store has 4 tensors including the text, embedding, ids, and  metadata which includes the filename of the text\n",
    "\n",
    " #  tensor      htype     shape    dtype  compression\n",
    " #  -------    -------   -------  -------  ------- \n",
    " #   text       text      (0,)      str     None   \n",
    " # metadata     json      (0,)      str     None   \n",
    " # embedding  embedding   (0,)    float32   None   \n",
    " #    id        text      (0,)      str     None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6c2a565-982b-4eae-81ef-31d9be227ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading data to deeplake dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:01<00:00, 21.35it/s]\n",
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://pavelkloscz/twitter_algorithm_twml_2', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      "   text       text      (24, 1)      str     None   \n",
      " metadata     json      (24, 1)      str     None   \n",
      " embedding  embedding  (24, 1536)  float32   None   \n",
      "    id        text      (24, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "# Create a LlamaIndex StorageContext and VectorStoreIndex, and use the from_documents() method to populate\n",
    "#   the Vector Store with data. This step takes several minutes because of the time to embed the text\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context,\n",
    ")\n",
    "\n",
    "# [OUTPUT] Vector Store has 8286 rows of data\n",
    " #  tensor      htype       shape       dtype  compression\n",
    " #  -------    -------     -------     -------  ------- \n",
    " #   text       text      (8262, 1)      str     None   \n",
    " # metadata     json      (8262, 1)      str     None   \n",
    " # embedding  embedding  (8262, 1536)  float32   None   \n",
    " #    id        text      (8262, 1)      str     None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ee8c39-faeb-475e-baab-ec8a7a51e353",
   "metadata": {},
   "source": [
    "#### Use the Vector Store in a Q&A App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abdd82a3-a3f8-4d72-9e8d-60f985c9486b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://pavelkloscz/twitter_algorithm_twml_2 already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "# Use the VectorStore in Q&A app, where the embeddings will be used to filter relevant documents (texts)\n",
    "#   that are fed into an LLM in order to answer a question.\n",
    "# If we were on another machine, we would load the existing Vector Store without re-ingesting the data\n",
    "\n",
    "vector_store = DeepLakeVectorStore(dataset_path=dataset_path, read_only=True)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea2463e3-ba27-4e4f-8fb4-9f70230f9f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LlamaIndex query engine and run a query\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70155e77-4ffb-472a-b871-b0664c7ff4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python\n"
     ]
    }
   ],
   "source": [
    "# response = query_engine.query(\"What programming language is most of the SimClusters written in?\")\n",
    "response = query_engine.query(\"What programming language is most of the Batch Prediction written in?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "761bb70f-559e-434c-b019-80c12ba81306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most of the TWML (TensorFlow Wrapper for Machine Learning) code is written in Python.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What programming language is most of the TWML written in?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "489ec74b-756e-4dcb-adba-daf483df417e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest Python function in these files is the `call` function within the `Sequential` class in the file `sequential.py`.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the longest Python function in these files?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7346f600-4895-4418-bf36-075a45ef7d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n__init__', '\\nadd', '\\npop', '\\ncall', '\\nget', '\\nget_output', '\\nget_layer_by_name', '\\nget_layer_output_by_name', '\\ncompute_output_shape']\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Give me list of all Python functions, they start by keyword def, return only function names as list?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dbb228c-9544-476a-923e-62bd5f7d8fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response=\"['\\\\n__init__', '\\\\nadd', '\\\\npop', '\\\\ncall', '\\\\nget', '\\\\nget_output', '\\\\nget_layer_by_name', '\\\\nget_layer_output_by_name', '\\\\ncompute_output_shape']\", source_nodes=[NodeWithScore(node=TextNode(id_='489998aa-9808-4048-b792-d9ac2e948bee', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Pavel\\\\projects\\\\ai-llm-deeplake\\\\Deep Lake\\\\02. Examples\\\\the-algorithm\\\\twml\\\\twml\\\\layers\\\\sequential.py', 'file_name': 'sequential.py', 'file_type': 'text/x-python', 'file_size': 4172, 'creation_date': '2025-03-13', 'last_modified_date': '2025-03-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e3d27174-dd83-466b-a0c6-9c83fbb7b89c', node_type='4', metadata={'file_path': 'C:\\\\Users\\\\Pavel\\\\projects\\\\ai-llm-deeplake\\\\Deep Lake\\\\02. Examples\\\\the-algorithm\\\\twml\\\\twml\\\\layers\\\\sequential.py', 'file_name': 'sequential.py', 'file_type': 'text/x-python', 'file_size': 4172, 'creation_date': '2025-03-13', 'last_modified_date': '2025-03-13'}, hash='35668e8bbc3607d004a69af91bdf8b694525cc3e5ef41db98e981aa8a7b75ee9')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='\"\"\"\\r\\nImplementing Sequential Layer container\\r\\n\"\"\"\\r\\n\\r\\n\\r\\nfrom .layer import Layer\\r\\n\\r\\nfrom tensorflow import keras\\r\\nfrom tensorflow.python.layers import base\\r\\n\\r\\n\\r\\nclass Sequential(Layer):\\r\\n  \"\"\"\\r\\n  A sequential stack of layers.\\r\\n\\r\\n  Arguments:\\r\\n      layers: list of layers to add to the model.\\r\\n\\r\\n  Output:\\r\\n      the output of the sequential layers\\r\\n   \"\"\"\\r\\n\\r\\n  def __init__(self, layers=None, **kwargs):\\r\\n    self._layers = []  # Stack of layers.\\r\\n    self._layer_names = []  # Stack of layers names\\r\\n    self._layer_outputs = []\\r\\n    # Add to the model any layers passed to the constructor.\\r\\n    if layers:\\r\\n      for layer in layers:\\r\\n        self.add(layer)\\r\\n    super(Sequential, self).__init__(**kwargs)\\r\\n\\r\\n  def add(self, layer):\\r\\n    \"\"\"Adds a layer instance on top of the layer stack.\\r\\n\\r\\n    Arguments:\\r\\n      layer:\\r\\n        layer instance.\\r\\n\\r\\n    Raises:\\r\\n      TypeError:\\r\\n        if the layer argument is not instance of base.Layer\\r\\n    \"\"\"\\r\\n    if not isinstance(layer, base.Layer) and not isinstance(layer, keras.layers.Layer):\\r\\n      raise TypeError(\\'The added layer must be an instance of class Layer\\')\\r\\n\\r\\n    if layer.name in self._layer_names:\\r\\n      raise ValueError(\\'Layer with name %s already exists in sequential layer\\' % layer.name)\\r\\n\\r\\n    self._layers.append(layer)\\r\\n    self._layer_names.append(layer.name)\\r\\n\\r\\n  def pop(self):\\r\\n    \"\"\"Removes the last layer in the model.\\r\\n\\r\\n    Raises:\\r\\n      TypeError:\\r\\n        if there are no layers in the model.\\r\\n    \"\"\"\\r\\n    if not self._layers or not self._layer_names:\\r\\n      raise TypeError(\\'There are no layers in the model.\\')\\r\\n    self._layers.pop()\\r\\n    self._layer_names.pop()\\r\\n\\r\\n  def call(self, inputs, **kwargs):  # pylint: disable=unused-argument\\r\\n    \"\"\"The logic of the layer lives here.\\r\\n\\r\\n    Arguments:\\r\\n      inputs:\\r\\n        input tensor(s).\\r\\n\\r\\n    Returns:\\r\\n      The output of the sequential layers\\r\\n    \"\"\"\\r\\n    self._layer_outputs = []\\r\\n    for layer in self._layers:\\r\\n      # don\\'t use layer.call because you want to build individual layers\\r\\n      inputs = layer(inputs)  # overwrites the current input after it has been processed\\r\\n      self._layer_outputs.append(inputs)\\r\\n    return inputs\\r\\n\\r\\n  @property\\r\\n  def layers(self):\\r\\n    \"\"\" Return the layers in the sequential layer \"\"\"\\r\\n    return self._layers\\r\\n\\r\\n  @property\\r\\n  def layer_names(self):\\r\\n    \"\"\" Return the layer names in the sequential layer \"\"\"\\r\\n    return self._layer_names\\r\\n\\r\\n  @property\\r\\n  def layer_outputs(self):\\r\\n    \"\"\" Return the layer outputs in the sequential layer \"\"\"\\r\\n    return self._layer_outputs\\r\\n\\r\\n  def get(self, key):\\r\\n    \"\"\"Retrieves the n-th layer.\\r\\n\\r\\n    Arguments:\\r\\n      key:\\r\\n        index of the layer\\r\\n\\r\\n    Output:\\r\\n      The n-th layer where n is equal to the key.\\r\\n    \"\"\"\\r\\n    return self._layers[key]\\r\\n\\r\\n  def get_output(self, key):\\r\\n    \"\"\"Retrieves the n-th layer output.\\r\\n\\r\\n    Arguments:\\r\\n      key:\\r\\n        index of the layer\\r\\n\\r\\n    Output:\\r\\n      The intermediary output equivalent to the nth layer, where n is equal to the key.\\r\\n    \"\"\"\\r\\n    return self._layer_outputs[key]\\r\\n\\r\\n  def get_layer_by_name(self, name):\\r\\n    \"\"\"Retrieves the layer corresponding to the name.\\r\\n\\r\\n    Arguments:\\r\\n      name:\\r\\n        name of the layer\\r\\n\\r\\n    Output:\\r\\n      list of layers that have the name desired\\r\\n    \"\"\"\\r\\n    return self._layers[self._layer_names.index(name)]\\r\\n\\r\\n  def get_layer_output_by_name(self, name):\\r\\n    \"\"\"Retrieves the layer output corresponding to the name.\\r\\n\\r\\n    Arguments:\\r\\n      name:\\r\\n        name of the layer\\r\\n\\r\\n    Output:\\r\\n      list of the output of the layers that have the desired name\\r\\n    \"\"\"\\r\\n    return self._layer_outputs[self._layer_names.index(name)]\\r\\n\\r\\n  @property\\r\\n  def init(self):\\r\\n    \"\"\" returns a list of initialization ops (one per layer) \"\"\"\\r\\n    return [layer.init for layer in self._layers]\\r\\n\\r\\n  def compute_output_shape(self, input_shape):\\r\\n    \"\"\"Computes the output shape of the layer given the input shape.\\r\\n\\r\\n    Args:\\r\\n      input_shape: A (possibly nested tuple of) `TensorShape`.  It need not\\r\\n        be fully defined (e.g. the batch size may be unknown).\\r\\n\\r\\n    Raise NotImplementedError.\\r\\n\\r\\n    \"\"\"\\r\\n    raise NotImplementedError', mimetype='text/plain', start_char_idx=0, end_char_idx=4170, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6734939217567444), NodeWithScore(node=TextNode(id_='06ad21aa-faa4-4b5b-bedb-6f87b565d6e4', embedding=None, metadata={'file_path': 'C:\\\\Users\\\\Pavel\\\\projects\\\\ai-llm-deeplake\\\\Deep Lake\\\\02. Examples\\\\the-algorithm\\\\twml\\\\twml\\\\layers\\\\partition.py', 'file_name': 'partition.py', 'file_type': 'text/x-python', 'file_size': 2194, 'creation_date': '2025-03-13', 'last_modified_date': '2025-03-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2dad0436-7d4b-459c-81a2-f7a28aec8fa0', node_type='4', metadata={'file_path': 'C:\\\\Users\\\\Pavel\\\\projects\\\\ai-llm-deeplake\\\\Deep Lake\\\\02. Examples\\\\the-algorithm\\\\twml\\\\twml\\\\layers\\\\partition.py', 'file_name': 'partition.py', 'file_type': 'text/x-python', 'file_size': 2194, 'creation_date': '2025-03-13', 'last_modified_date': '2025-03-13'}, hash='cc60f6c244f8dbe3affbf0c4ccf197d823bfc97b59e377920ddb5369dc03c387')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='\"\"\"\\r\\nImplementing partition Layer\\r\\n\"\"\"\\r\\n\\r\\n\\r\\nfrom .layer import Layer\\r\\n\\r\\nimport tensorflow.compat.v1 as tf\\r\\n\\r\\n\\r\\nclass Partition(Layer):\\r\\n  \"\"\"\\r\\n  This layer implements:\\r\\n\\r\\n  .. code-block:: python\\r\\n\\r\\n    tf.dynamic_partition(input_vals, partition_ids, self.partitions)\\r\\n\\r\\n  Input:\\r\\n    partitions:\\r\\n      the number of partitions which we will divide the hashmap keys/bvalues\\r\\n\\r\\n  Output:\\r\\n    A layer that performs partitioning\\r\\n   \"\"\"\\r\\n\\r\\n  def __init__(self, partitions=2, **kwargs):\\r\\n    self.partitions = partitions\\r\\n    super(Partition, self).__init__(**kwargs)\\r\\n\\r\\n  def compute_output_shape(self, input_shape):\\r\\n    \"\"\"Computes the output shape of the layer given the input shape.\\r\\n\\r\\n    Args:\\r\\n      input_shape: A (possibly nested tuple of) `TensorShape`.  It need not\\r\\n        be fully defined (e.g. the batch size may be unknown).\\r\\n\\r\\n    Raises NotImplementedError.\\r\\n\\r\\n    \"\"\"\\r\\n    raise NotImplementedError\\r\\n\\r\\n  def call(self, partition_ids, input_vals, input_keys, **kwargs):\\r\\n    \"\"\"This layer is responsible for partitioning the values/keys of a hashmap\\r\\n\\r\\n    Arguments:\\r\\n      partition_ids:\\r\\n        Tensor that is equivalent to boolean (int32).\\r\\n      input_vals:\\r\\n        Tensor that represents the values of the hashmap(float).\\r\\n      input_keys:\\r\\n        Tensor that represents the keys of the hashmap(float)\\r\\n\\r\\n    Returns:\\r\\n      The output of the partition layer, which is a list of lists which looks\\r\\n      something like:\\r\\n\\r\\n      .. code-block:: python\\r\\n\\r\\n        [[vals_0, vals_1], [keys_0, keys_1], [indices_0, indices_1]]\\r\\n\\r\\n      where:\\r\\n        vals_x:\\r\\n          values of the hashmap for partition x\\r\\n        keys_x:\\r\\n          keys of the hashmap for partition x\\r\\n        indices_x:\\r\\n          indices of the hashmap for partition x\\r\\n    \"\"\"\\r\\n    partioned_val = tf.dynamic_partition(input_vals, partition_ids, self.partitions)\\r\\n    partioned_keys = tf.dynamic_partition(input_keys, partition_ids, self.partitions)\\r\\n    partioned_indices = tf.dynamic_partition(tf.range(tf.shape(partition_ids)[0]),\\r\\n                                             tf.cast(partition_ids, tf.int32), self.partitions)\\r\\n    return [partioned_val, partioned_keys, partioned_indices]', mimetype='text/plain', start_char_idx=0, end_char_idx=2192, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6675460338592529)], metadata={'489998aa-9808-4048-b792-d9ac2e948bee': {'file_path': 'C:\\\\Users\\\\Pavel\\\\projects\\\\ai-llm-deeplake\\\\Deep Lake\\\\02. Examples\\\\the-algorithm\\\\twml\\\\twml\\\\layers\\\\sequential.py', 'file_name': 'sequential.py', 'file_type': 'text/x-python', 'file_size': 4172, 'creation_date': '2025-03-13', 'last_modified_date': '2025-03-13'}, '06ad21aa-faa4-4b5b-bedb-6f87b565d6e4': {'file_path': 'C:\\\\Users\\\\Pavel\\\\projects\\\\ai-llm-deeplake\\\\Deep Lake\\\\02. Examples\\\\the-algorithm\\\\twml\\\\twml\\\\layers\\\\partition.py', 'file_name': 'partition.py', 'file_type': 'text/x-python', 'file_size': 2194, 'creation_date': '2025-03-13', 'last_modified_date': '2025-03-13'}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e93cfc-0698-4dca-a650-1f0f3671f09b",
   "metadata": {},
   "source": [
    "## Setting OpenAI Model for LlamaIndex (GitHub Copilot) CHECK-TRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6ce8d37-f48f-4386-a3d7-4c80087d20fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To set a specific OpenAI model for use with LlamaIndex, you can configure it when creating the LLM object\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Create an OpenAI LLM instance with your desired model\n",
    "# llm = OpenAI(model=\"gpt-4\", temperature=0.1, api_key=\"your-openai-api-key\")\n",
    "llm = OpenAI(model=MODEL_GPT)\n",
    "\n",
    "# Use this LLM with your LlamaIndex components\n",
    "# For example, when creating a query engine:\n",
    "from llama_index.core import VectorStoreIndex\n",
    "query_engine = index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf94d6fe-90ad-4629-a8de-0d5d06f482fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ServiceContext is deprecated. Use llama_index.settings.Settings instead, or pass in modules to local functions/methods/interfaces.\nSee the docs for updated usage/migration: \nhttps://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m llm = OpenAI(model=MODEL_GPT)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Create service context with this LLM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m service_context = \u001b[43mServiceContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Use service context when creating your index\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VectorStoreIndex\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\projects\\ai-llm-deeplake\\.venv\\Lib\\site-packages\\llama_index\\core\\service_context.py:31\u001b[39m, in \u001b[36mServiceContext.from_defaults\u001b[39m\u001b[34m(cls, **kwargs)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_defaults\u001b[39m(\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m     23\u001b[39m     **kwargs: Any,\n\u001b[32m     24\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mServiceContext\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     25\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a ServiceContext from defaults.\u001b[39;00m\n\u001b[32m     26\u001b[39m \n\u001b[32m     27\u001b[39m \u001b[33;03m    NOTE: Deprecated, use llama_index.settings.Settings instead or pass in\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03m    modules to local functions/methods/interfaces.\u001b[39;00m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     32\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mServiceContext is deprecated. Use llama_index.settings.Settings instead, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     33\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor pass in modules to local functions/methods/interfaces.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     34\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSee the docs for updated usage/migration: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     36\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: ServiceContext is deprecated. Use llama_index.settings.Settings instead, or pass in modules to local functions/methods/interfaces.\nSee the docs for updated usage/migration: \nhttps://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/"
     ]
    }
   ],
   "source": [
    "# You can specify any OpenAI model like \"gpt-3.5-turbo\", \"gpt-4-turbo\", or \"gpt-4o\"\n",
    "#   when creating the OpenAI instance.\n",
    "# If you're using a ServiceContext for broader configuration\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import ServiceContext\n",
    "\n",
    "# Create LLM with desired model\n",
    "# llm = OpenAI(model=\"gpt-4\")\n",
    "llm = OpenAI(model=MODEL_GPT)\n",
    "\n",
    "# Create service context with this LLM\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "\n",
    "# Use service context when creating your index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45426d24-b8fc-4d2f-bee8-764a498c20ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated LlamaIndex Code with Settings\n",
    "\n",
    "# [Migrating from ServiceContext to Settings]\n",
    "# - https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/\n",
    "# [Configuring Settings]\n",
    "# - https://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/settings/\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "\n",
    "# Create LLM with desired model\n",
    "# llm = OpenAI(model=\"gpt-4\")\n",
    "llm = OpenAI(model=MODEL_GPT)\n",
    "\n",
    "# Configure global settings\n",
    "Settings.llm = llm\n",
    "# Optionally set other settings\n",
    "# Settings.embed_model = \"...\"\n",
    "# Settings.chunk_size = 1024\n",
    "\n",
    "# Using the configured settings when creating your index\n",
    "# No need to pass settings explicitly, it uses the global settings\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Alternatively, if you want to use settings for just this index:\n",
    "# index = VectorStoreIndex.from_documents(documents, settings=Settings(llm=llm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
