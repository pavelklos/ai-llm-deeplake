{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93495a2e-3e04-4cb8-81d7-a00f9dc0b4f4",
   "metadata": {},
   "source": [
    "# EXAMPLES (RAG)\n",
    "- [RAG](https://docs.activeloop.ai/examples/rag)\n",
    "  - [RAG Quickstart](https://docs.activeloop.ai/examples/rag/quickstart)\n",
    "  - [RAG Tutorials](https://docs.activeloop.ai/examples/rag/tutorials)\n",
    "    - [**Vector Store Basics**](https://docs.activeloop.ai/examples/rag/tutorials/vector-store-basics)\n",
    "    - [Vector Search Options](https://docs.activeloop.ai/examples/rag/tutorials/vector-search-options)\n",
    "      - [LangChain API](https://docs.activeloop.ai/examples/rag/tutorials/vector-search-options/langchain-api)\n",
    "      - [Deep Lake Vector Store API](https://docs.activeloop.ai/examples/rag/tutorials/vector-search-options/vector-store-api)\n",
    "      - [Managed Database REST API](https://docs.activeloop.ai/examples/rag/tutorials/vector-search-options/rest-api)\n",
    "    - [Customizing Your Vector Store](https://docs.activeloop.ai/examples/rag/tutorials/step-4-customizing-vector-stores)\n",
    "    - [Image Similarity Search](https://docs.activeloop.ai/examples/rag/tutorials/image-similarity-search)\n",
    "    - [Improving Search Accuracy using Deep Memory](https://docs.activeloop.ai/examples/rag/tutorials/deepmemory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9134e12-66df-4b48-915d-a3b9f5e1c550",
   "metadata": {},
   "source": [
    "## RAG Tutorials (Vector Store Basics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3217cdbc-a21f-4052-9207-19ce8390dc84",
   "metadata": {},
   "source": [
    "### Downloading and Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f039121d-b446-48a8-8db8-3db88e9be257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavel\\projects\\ai-llm-deeplake\\.venv\\Lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (4.1.13) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from deeplake.core.vectorstore import VectorStore\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override = True)\n",
    "open_api_key = os.getenv('OPENAI_API_KEY')\n",
    "activeloop_token = os.getenv('ACTIVELOOP_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e2465a8-5dce-4b7a-9a39-f1dda661fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/twitter/the-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb18f9c-b21b-454c-ab14-daf03e197b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store_path = '/vector_store_getting_started'\n",
    "# repo_path = '/the-algorithm'\n",
    "\n",
    "vector_store_path = './vector_store_getting_started'\n",
    "# repo_path = './the-algorithm'\n",
    "repo_path = './the-algorithm/twml/twml/layers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4ab7e4d-68fa-4e6d-8ac6-9173a1381fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_prediction_tensor_writer.py\n",
      "batch_prediction_writer.py\n",
      "data_record_tensor_writer.py\n",
      "full_dense.py\n",
      "full_sparse.py\n",
      "isotonic.py\n",
      "layer.py\n",
      "mdl.py\n",
      "partition.py\n",
      "percentile_discretizer.py\n",
      "sequential.py\n",
      "sparse_max_norm.py\n",
      "stitch.py\n",
      "__init__.py\n"
     ]
    }
   ],
   "source": [
    "CHUNK_SIZE = 1000\n",
    "\n",
    "chunked_text = []\n",
    "metadata = []\n",
    "for dirpath, dirnames, filenames in os.walk(repo_path):\n",
    "    for file in filenames:\n",
    "        try: \n",
    "            full_path = os.path.join(dirpath,file)\n",
    "            with open(full_path, 'r') as f:\n",
    "               text = f.read()\n",
    "            new_chunkned_text = [text[i:i+CHUNK_SIZE] for i in range(0,len(text), CHUNK_SIZE)]\n",
    "            chunked_text += new_chunkned_text\n",
    "            metadata += [{'filepath': full_path} for i in range(len(new_chunkned_text))]\n",
    "            print(file)  # TODO: COMMENT\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aed35d2-2555-444b-9458-c91705fbf6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "print(type(chunked_text))\n",
    "print(len(chunked_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7940b72b-99da-46bc-a359-401542cfabb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# pylint: disable=no-member, invalid-name\n",
      "\"\"\"\n",
      "Implementing Writer Layer\n",
      "\"\"\"\n",
      "from .layer import Layer\n",
      "\n",
      "import libtwml\n",
      "\n",
      "\n",
      "class BatchPredictionTensorWriter(Layer):\n",
      "  \"\"\"\n",
      "  A layer that packages keys and \n"
     ]
    }
   ],
   "source": [
    "# chunked_text[0]\n",
    "# print(chunked_text[0])\n",
    "print(chunked_text[0][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "864bfa75-6873-49a6-ba28-6548f4bce442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_function(texts, model=\"text-embedding-ada-002\"):\n",
    "   \n",
    "   if isinstance(texts, str):\n",
    "       texts = [texts]\n",
    "\n",
    "   texts = [t.replace(\"\\n\", \" \") for t in texts]\n",
    "   \n",
    "   return [data.embedding for data in openai.embeddings.create(input = texts, model=model).data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6237315a-5c5e-40f4-8f4e-7556cd62cfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in ./vector_store_getting_started already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 76 embeddings in 1 batches of size 76:: 100%|███████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./vector_store_getting_started', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      " embedding  embedding  (152, 1536)  float32   None   \n",
      "    id        text      (152, 1)      str     None   \n",
      " metadata     json      (152, 1)      str     None   \n",
      "   text       text      (152, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vector_store = VectorStore(\n",
    "    path = vector_store_path,\n",
    ")\n",
    "\n",
    "vector_store.add(text = chunked_text, \n",
    "                 embedding_function = embedding_function, \n",
    "                 embedding_data = chunked_text, \n",
    "                 metadata = metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9e979cd-46b6-47f7-8c96-3444a5d211e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='./vector_store_getting_started', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype       shape      dtype  compression\n",
      "  -------    -------     -------    -------  ------- \n",
      " embedding  embedding  (152, 1536)  float32   None   \n",
      "    id        text      (152, 1)      str     None   \n",
      " metadata     json      (152, 1)      str     None   \n",
      "   text       text      (152, 1)      str     None   \n"
     ]
    }
   ],
   "source": [
    "vector_store.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c9b84-09cb-4d8e-8a5c-e9ca20c147ab",
   "metadata": {},
   "source": [
    "### Performing Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c43ce1f2-36c4-4fe8-a159-d8e80078b69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"What do trust and safety models do?\"\n",
    "\n",
    "# Check file [mdl.py] and its text 'MDL layer is constructed by MDLCalibrator'\n",
    "# prompt = \"MDL layer is constructed by?\"\n",
    "# prompt = \"How is the MDL layer constructed?\"\n",
    "prompt = \"What is the MDL layer?\"\n",
    "\n",
    "search_results = vector_store.search(embedding_data=prompt, embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "083ce60f-ee16-403b-b06f-28f2c40c5ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_results['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "235428a2-9e3f-468f-840d-36a6c206fba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# pylint: disable=no-member, attribute-defined-outside-init, too-many-instance-attributes\n",
      "\"\"\"\n",
      "Implementing MDL Layer\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "from .layer import Layer\n",
      "from .partition import Partition\n",
      "from .stitch import Stitch\n",
      "\n",
      "import libtwml\n",
      "import numpy as np\n",
      "import tensorflow.compat.v1 as tf\n",
      "import twml\n",
      "\n",
      "\n",
      "class MDL(Layer):  # noqa: T000\n",
      "  \"\"\"\n",
      "  MDL layer is constructed by MDLCalibrator after accumulating data\n",
      "  and performing minimum description length (MDL) calibration.\n",
      "\n",
      "  MDL takes sparse continuous features and converts then to sparse\n",
      "  binary features. Each binary output feature is associated to an MDL bin.\n",
      "  Each MDL input feature is converted to n_bin bins.\n",
      "  Each MDL calibration tries to find bin delimiters such that the number of features values\n",
      "  per bin is roughly equal (for each given MDL feature).\n",
      "  Note that if an input feature is rarely used, so will its associated output bin/features.\n",
      "  \"\"\"\n",
      "\n",
      "  def __init__(\n",
      "          self,\n",
      "          n_feature, n_bin, out_bits,\n",
      "          bin_values=None,\n"
     ]
    }
   ],
   "source": [
    "# search_results['text']\n",
    "print(search_results['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "752a47c0-3250-4be5-9bf1-8712a8eafb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filepath': './the-algorithm/twml/twml/layers\\\\mdl.py'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results['metadata'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7a3426-3a47-49dc-bf4e-bcc20eb40ab6",
   "metadata": {},
   "source": [
    "### Customization of Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e3c3757-0c4a-45aa-bcac-d03bb6148114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize your vector search with simple parameters,\n",
    "#   such as selecting the distance_metric and top k results\n",
    "\n",
    "search_results = vector_store.search(embedding_data=prompt, \n",
    "                                     embedding_function=embedding_function, \n",
    "                                     k=10,\n",
    "                                     distance_metric='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64638841-d283-450a-9928-95bf1395a91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_results['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2babe324-471a-4755-af75-e1489fa68f2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# pylint: disable=no-member, attribute-defined-outside-init, too-many-instance-attributes\n",
      "\"\"\"\n",
      "Implementing MDL Layer\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "from .layer import Layer\n",
      "from .partition import Partition\n",
      "from .stitch import Stitch\n",
      "\n",
      "import libtwml\n",
      "import numpy as np\n",
      "import tensorflow.compat.v1 as tf\n",
      "import twml\n",
      "\n",
      "\n",
      "class MDL(Layer):  # noqa: T000\n",
      "  \"\"\"\n",
      "  MDL layer is constructed by MDLCalibrator after accumulating data\n",
      "  and performing minimum description length (MDL) calibration.\n",
      "\n",
      "  MDL takes sparse continuous features and converts then to sparse\n",
      "  binary features. Each binary output feature is associated to an MDL bin.\n",
      "  Each MDL input feature is converted to n_bin bins.\n",
      "  Each MDL calibration tries to find bin delimiters such that the number of features values\n",
      "  per bin is roughly equal (for each given MDL feature).\n",
      "  Note that if an input feature is rarely used, so will its associated output bin/features.\n",
      "  \"\"\"\n",
      "\n",
      "  def __init__(\n",
      "          self,\n",
      "          n_feature, n_bin, out_bits,\n",
      "          bin_values=None,\n"
     ]
    }
   ],
   "source": [
    "# search_results['text'][0]\n",
    "print(search_results['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a79fa19-8f43-4b1d-9af1-bd16a288bcc8",
   "metadata": {},
   "source": [
    "### Full Customization of Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "398f1035-23d4-4dcd-a3b0-bd6661248f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://activeloop/twitter-algorithm already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "# Load a representative Vector Store that is already stored in  Deep Lake Tensor Database\n",
    "\n",
    "vector_store = VectorStore(\n",
    "    path = \"hub://activeloop/twitter-algorithm\",\n",
    "    read_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bfc0dc0-f1e1-4831-ba83-dcf2de6435bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query should be constructed using the Tensor Query Language (TQL) syntax\n",
    "\n",
    "prompt = \"What do trust and safety models do?\"\n",
    "\n",
    "embedding = embedding_function(prompt)[0]\n",
    "\n",
    "# Format the embedding array or list as a string, so it can be passed in the REST API request.\n",
    "embedding_string = \",\".join([str(item) for item in embedding])\n",
    "\n",
    "tql_query = f\"select * from (select text, cosine_similarity(embedding, ARRAY[{embedding_string}]) as score) order by score desc limit 5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdaf9012-b507-4914-ace8-343a94fb9d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the query, noting that the query execution happens in the Managed Tensor Database, and not on the client\n",
    "\n",
    "search_results = vector_store.search(query=tql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36d40855-38fe-433f-ad70-4f81ae67ad78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trust and Safety Models\n",
      "=======================\n",
      "\n",
      "We decided to open source the training code of the following models:\n",
      "- pNSFWMedia: Model to detect tweets with NSFW images. This includes adult and porn content.\n",
      "- pNSFWText: Model to detect tweets with NSFW text, adult/sexual topics.\n",
      "- pToxicity: Model to detect toxic tweets. Toxicity includes marginal content like insults and certain types of harassment. Toxic content does not violate Twitter's terms of service.\n",
      "- pAbuse: Model to detect abusive content. This includes violations of Twitter's terms of service, including hate speech, targeted harassment and abusive behavior.\n",
      "\n",
      "We have several more models and rules that we are not going to open source at this time because of the adversarial nature of this area. The team is considering open sourcing more models going forward and will keep the community posted accordingly.\n"
     ]
    }
   ],
   "source": [
    "# search_results['text'][0]\n",
    "print(search_results['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1f12a9d-9118-4726-a6d0-4b9bdf786e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(search_results['metadata'][0])\n",
    "\n",
    "# Returns {'filepath': '/Users/istranic/ActiveloopCode/the-algorithm/trust_and_safety_models/README.md', 'extension': '.md'}\n",
    "# ERROR: KeyError: 'metadata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21b4f9b7-a98f-4cd1-8662-c67ba982d9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'score']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(search_results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b57955e-a3a6-4e13-b2a7-5b832d474578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8414647579193115\n"
     ]
    }
   ],
   "source": [
    "print(search_results['score'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d65b003-3805-41f7-9ac3-7b4c35e5728d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [\"Trust and Safety Models\\n=======================\\n\\nWe decided to open source the training code of the following models:\\n- pNSFWMedia: Model to detect tweets with NSFW images. This includes adult and porn content.\\n- pNSFWText: Model to detect tweets with NSFW text, adult/sexual topics.\\n- pToxicity: Model to detect toxic tweets. Toxicity includes marginal content like insults and certain types of harassment. Toxic content does not violate Twitter's terms of service.\\n- pAbuse: Model to detect abusive content. This includes violations of Twitter's terms of service, including hate speech, targeted harassment and abusive behavior.\\n\\nWe have several more models and rules that we are not going to open source at this time because of the adversarial nature of this area. The team is considering open sourcing more models going forward and will keep the community posted accordingly.\", 'private lazy val thriftToModelMap: Map[ThriftSafetyLevel, SafetyLevel] = Map(\\n    ThriftSafetyLevel.AccessInternalPromotedContent -> AccessInternalPromotedContent,\\n    ThriftSafetyLevel.AdsBusinessSettings -> AdsBusinessSettings,\\n    ThriftSafetyLevel.AdsCampaign -> AdsCampaign,\\n    ThriftSafetyLevel.AdsManager -> AdsManager,\\n    ThriftSafetyLevel.AdsReportingDashboard -> AdsReportingDashboard,\\n    ThriftSafetyLevel.AllSubscribedLists -> AllSubscribedLists,\\n    ThriftSafetyLevel.Appeals -> Appeals,\\n    ThriftSafetyLevel.ArticleTweetTimeline -> ArticleTweetTimeline,\\n    ThriftSafetyLevel.BaseQig -> BaseQig,\\n    ThriftSafetyLevel.BirdwatchNoteAuthor -> BirdwatchNoteAuthor,\\n    ThriftSafetyLevel.BirdwatchNoteTweetsTimeline -> BirdwatchNoteTweetsTimeline,\\n    ThriftSafetyLevel.BirdwatchNeedsYourHelpNotifications -> BirdwatchNeedsYourHelpNotifications,\\n    ThriftSafetyLevel.BlockMuteUsersTimeline -> BlockMuteUsersTimeline,\\n    ThriftSafetyLevel.BrandSafety -> BrandSafety,\\n    ThriftSafetyLevel.CardPollVoting -> CardPollVoting,\\n    ThriftSafetyLevel.CardsService -> CardsService,\\n    ThriftSafetyLevel.Communities -> Communities,\\n    ThriftSafetyLevel.ContentControlToolInstall -> ContentControlToolInstall,\\n    ThriftSafetyLevel.ConversationFocalPrehydration -> ConversationFocalPrehydration,\\n    ThriftSafetyLevel.ConversationFocalTweet -> ConversationFocalTweet,\\n    ThriftSafetyLevel.ConversationInjectedTweet -> ConversationInjectedTweet,\\n    ThriftSafetyLevel.ConversationReply -> ConversationReply,\\n    ThriftSafetyLevel.CuratedTrendsRepresentativeTweet -> CuratedTrendsRepresentativeTweet,\\n    ThriftSafetyLevel.CurationPolicyViolations -> CurationPolicyViolations,\\n    ThriftSafetyLevel.DevPlatformGetListTweets -> DevPlatformGetListTweets,\\n    ThriftSafetyLevel.DesFollowingAndFollowersUserList -> DesFollowingAndFollowersUserList,\\n    ThriftSafetyLevel.DesHomeTimeline -> DesHomeTimeline,\\n    ThriftSafetyLevel.DesQuoteTweetTimeline -> DesQuoteTweetTimeline,\\n    ThriftSafetyLevel.DesRealtime -> DesRealtime,\\n    ThriftSafetyLevel.DesRealtimeSpamEnrichment -> DesRealtimeSpamEnrichment,\\n    ThriftSafetyLevel.DesRealtimeTweetFilter -> DesRealtimeTweetFilter,\\n    ThriftSafetyLevel.DesRetweetingUsers -> DesRetweetingUsers,\\n    ThriftSafetyLevel.DesTweetDetail -> DesTweetDetail,\\n    ThriftSafetyLevel.DesTweetLikingUsers -> DesTweetLikingUsers,\\n    ThriftSafetyLevel.DesUserBookmarks -> DesUserBookmarks,\\n    ThriftSafetyLevel.DesUserLikedTweets -> DesUserLikedTweets,\\n    ThriftSafetyLevel.DesUserMentions -> DesUserMentions,\\n    ThriftSafetyLevel.DesUserTweets -> DesUserTweets,\\n    ThriftSafetyLevel.DevPlatformComplianceStream -> DevPlatformComplianceStream,\\n    ThriftSafetyLevel.DirectMessages -> DirectMessages,\\n    ThriftSafetyLevel.DirectMessagesConversationList -> DirectMessagesConversationList,\\n    ThriftSafetyLevel.DirectMessagesConversationTimeline -> DirectMessagesConversationTimeline,\\n    ThriftSafetyLevel.DirectMessagesInbox -> DirectMessagesInbox,\\n    ThriftSafetyLevel.DirectMessagesMutedUsers -> DirectMessagesMutedUsers,\\n    ThriftSafetyLevel.DirectMessagesPinned -> DirectMessagesPinned,\\n    ThriftSafetyLevel.DirectMessagesSearch -> DirectMessagesSearch,\\n    ThriftSafetyLevel.EditHistoryTimeline -> EditHistoryTimeline,\\n    ThriftSafetyLevel.ElevatedQuoteTweetTimeline -> ElevatedQuoteTweetTimeline,\\n    ThriftSafetyLevel.EmbeddedTweet -> EmbeddedTweet,\\n    ThriftSafetyLevel.EmbedsPublicInterestNotice -> EmbedsPublicInterestNotice,\\n    ThriftSafetyLevel.EmbedTweetMarkup -> EmbedTweetMarkup,\\n    ThriftSafetyLevel.ExploreRecommendations -> ExploreRecommendations,\\n    ThriftSafetyLevel.WritePathLimitedActionsEnforcement -> WritePathLimitedActionsEnforcement,\\n    ThriftSafetyLevel.FilterAll -> FilterAll,\\n    ThriftSafetyLevel.FilterAllPlaceholder -> FilterAllPlaceholder,\\n    ThriftSafetyLevel.FilterDefault -> FilterDefault,\\n    ThriftSafetyLevel.FilterNone -> FilterNone,\\n    ThriftSafetyLevel.FollowedTopicsTimeline -> FollowedTopicsTimeline,', 'private lazy val modelToThriftMap: Map[SafetyLevel, ThriftSafetyLevel] =\\n    for ((k, v) <- thriftToModelMap) yield (v, k)', 'In summary, the SALSA Candidate Source provides an account expansion based on the SALSA PYMK algorithm, utilizing a bipartite graph with personalized random walks to identify the most relevant and interesting recommendations for the user.', 'A primary labeling mechanism for Safety. A labeled entity associates with tweet, user, Direct Message, media, space etc. Safety labels power different ways of remediations (e.g. applying a SafetyLabel that results in tweet interstitial or notice).\\n\\nSafetyLabelType\\n===============\\n\\nDescribes a particular policy violation for a given noun instance, and usually leads to reduced visibility of the\\nlabeled entity in product surfaces. There are many deprecated, and experimental safety label types. Labels with these safety label types have no effect on VF. Additionally, some safety label types are not used, and not designed for VF.'], 'score': [0.8414647579193115, 0.7670316696166992, 0.7640699744224548, 0.762036144733429, 0.7598060965538025]}\n"
     ]
    }
   ],
   "source": [
    "print(search_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
