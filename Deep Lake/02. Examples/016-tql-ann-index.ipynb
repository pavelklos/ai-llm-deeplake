{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93495a2e-3e04-4cb8-81d7-a00f9dc0b4f4",
   "metadata": {},
   "source": [
    "# EXAMPLES (TQL)\n",
    "- [Tensor Query Language (TQL)](https://docs.activeloop.ai/examples/tql)\n",
    "  - [TQL Syntax](https://docs.activeloop.ai/examples/tql/syntax)\n",
    "  - [**Index for ANN Search**](https://docs.activeloop.ai/examples/tql/ann-index)\n",
    "    - [**Caching and Optimization**](https://docs.activeloop.ai/examples/tql/ann-index/caching-and-optimization)\n",
    "  - [Sampling Datasets](https://docs.activeloop.ai/examples/tql/sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b359eee8-d7e0-46e2-80cd-850dedd88d6c",
   "metadata": {},
   "source": [
    "## Index for ANN Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99afec2-dbae-4487-b4aa-cde0390c5dca",
   "metadata": {},
   "source": [
    "### Deep Lake Implements an Index for ANN Search\n",
    "*Deep Lake implements the Hierarchical Navigable Small World (HSNW) index for Approximate Nearest Neighbor (ANN) search. The index is based on the OSS Hsnwlib package with added optimizations. The implementation enables users to run queries on >35M embeddings in less than 1 second.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee8ffe-e63c-4aaa-ada0-328ae8b3233e",
   "metadata": {},
   "source": [
    "#### Unique aspects of Deep Lake's HSNW implementation\n",
    "- *Rapid index creation with multi-threading optimized for Deep Lake*\n",
    "- *Efficient memory management that reduces RAM usage*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2035ef83-1a08-4416-bf4e-620240080b70",
   "metadata": {},
   "source": [
    "#### Memory Management in Deep Lake\n",
    "**RAM Cost  >>  On-disk Cost  >>  Object Storage Cost**<br>\n",
    "*Minimizing RAM usage and maximizing object store significantly reduces costs of running a Vector Database. Deep Lake has a unique implementation of memory allocation that minimizes RAM requirement without any performance penalty:*<br>\n",
    "[Memory Architecture for the Deep Lake Vector Store](https://docs.activeloop.ai/~gitbook/image?url=https%3A%2F%2Fcontent.gitbook.com%2Fcontent%2FWOs95B2h3lcO4dwXDRJ3%2Fblobs%2Fa8UIj0ohjpnKm98rkz8X%2FIndex_Memory_Usage.png&width=400&dpr=3&quality=100&sign=4e7045b9&sv=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02f492e-c8f0-47c3-9b73-5c5af6e34fcd",
   "metadata": {},
   "source": [
    "#### Using the Index\n",
    "*By default, Deep Lake performs linear embedding search for up to 100,000 rows of data. Once the data exceeds that limit, the index is created and the embedding search uses ANN.  This index threshold is chosen because linear search is the most accurate, and for less than 100,000 rows, there is almost no performance penalty compared to ANN search.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c868c83-482a-4344-9a12-3e68a6436de9",
   "metadata": {},
   "source": [
    "*You may update the threshold for creating the index using the API below:*\n",
    "```\n",
    "vectorstore = VectorStore(path, index_params = {threshold: <threshold_int>})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517dbb49-ad38-4a7d-a767-ec88a23b9ce0",
   "metadata": {},
   "source": [
    "#### Limitations\n",
    "*The following limitations of the index are being implemented in upcoming releases:*<br>\n",
    "*If the search is performed using a combination of attribute and vector search, the index is not used and linear search is applied instead.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc0bd21-55ac-442e-b08a-a2a5b83c1916",
   "metadata": {},
   "source": [
    "## Caching and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7715f010-f46d-47f5-ac77-53a957e33242",
   "metadata": {},
   "source": [
    "### Extract Maximum Performance from Your Vector Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b18b48-ca73-4d9f-8785-0ecf6c794ceb",
   "metadata": {},
   "source": [
    "#### Tuning the Index Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d21b8e5-b666-4f34-9192-ea75b62abfc3",
   "metadata": {},
   "source": [
    "*The parameters of the HSNW index can be tuned using the index_params shown below:*\n",
    "```\n",
    "vectorstore = VectorStore(path, \n",
    "                          index_params = {\"threshold\": -1,\n",
    "                                          \"distance_metric\":\"COS\",\n",
    "                                          \"additional_params\": {\n",
    "                                              \"efConstruction\": 600,\n",
    "                                              \"M\": 32}})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3628df-8681-4bf7-a55b-f892bf5ef7e2",
   "metadata": {},
   "source": [
    "#### Caching of Embeddings and Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8218ec64-edc3-46dc-ba88-02ade9ed5038",
   "metadata": {},
   "source": [
    "*Either of the following operations caches the embeddings are on disk and the index in RAM:*<br>\n",
    "\n",
    "- *The index is created*\n",
    "- *The first vector search is executed after the Vector Store is loaded*<br>\n",
    "\n",
    "*Since the first query caches critical information, subsequent queries will execute much faster compared to the first query. Since the cache is invalidated after the Vector Store is loaded or initialized, the optimal access pattern is not to re-load the Vector Store each search, unless you believe it was updated by another client.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc91bf09-0a95-4d2d-800a-5d25a27578db",
   "metadata": {},
   "source": [
    "*The embeddings are cached on disk in the following locations:*\n",
    "```\n",
    "Mac: /tmp/....\n",
    "Linux: /var/folders/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e577e46-9b04-4619-aaca-f56f7f2aa24e",
   "metadata": {},
   "source": [
    "#### Caching of Other Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf5ced-af94-414c-962e-2f547f8a4ec4",
   "metadata": {},
   "source": [
    "*Tensors containing other information such as text and metadata are also cached in memory when they are used in queries. As a result, the first query that utilized this data will be slowest, with subsequent queries running much faster.*<br> \n",
    "\n",
    "*If the data size exceeds the cache size, it will be re-fetched with every query, thus reducing query performance. The default cache size is 2 MB, and you may increase the cache size using the parameter below:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5756334c-1d3b-494e-a210-bd6ad4616b0c",
   "metadata": {},
   "source": [
    "```\n",
    "vectorstore = VectorStore(path, memory_cache_size = <cache_in_MB))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
