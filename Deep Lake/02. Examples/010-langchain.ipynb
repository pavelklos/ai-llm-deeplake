{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93495a2e-3e04-4cb8-81d7-a00f9dc0b4f4",
   "metadata": {},
   "source": [
    "# EXAMPLES (RAG)\n",
    "- [RAG](https://docs.activeloop.ai/examples/rag)\n",
    "  - [RAG Quickstart](https://docs.activeloop.ai/examples/rag/quickstart)\n",
    "  - [RAG Tutorials](https://docs.activeloop.ai/examples/rag/tutorials)\n",
    "    - [Vector Store Basics](https://docs.activeloop.ai/examples/rag/tutorials/vector-store-basics)\n",
    "    - [Vector Search Options](https://docs.activeloop.ai/examples/rag/tutorials/vector-search-options)\n",
    "      - [LangChain API](https://docs.activeloop.ai/examples/rag/tutorials/vector-search-options/langchain-api)\n",
    "      - [Deep Lake Vector Store API](https://docs.activeloop.ai/examples/rag/tutorials/vector-search-options/vector-store-api)\n",
    "      - [Managed Database REST API](https://docs.activeloop.ai/examples/rag/tutorials/vector-search-options/rest-api)\n",
    "    - [Customizing Your Vector Store](https://docs.activeloop.ai/examples/rag/tutorials/step-4-customizing-vector-stores)\n",
    "    - [Image Similarity Search](https://docs.activeloop.ai/examples/rag/tutorials/image-similarity-search)\n",
    "    - [Improving Search Accuracy using Deep Memory](https://docs.activeloop.ai/examples/rag/tutorials/deepmemory)\n",
    "  - [**LangChain Integration**](https://docs.activeloop.ai/examples/rag/langchain-integration)\n",
    "  - [LlamaIndex Integration](https://docs.activeloop.ai/examples/rag/llamaindex-integration)\n",
    "  - [Managed Tensor Database](https://docs.activeloop.ai/examples/rag/managed-database)\n",
    "    - [REST API](https://docs.activeloop.ai/examples/rag/managed-database/rest-api)\n",
    "    - [Migrating Datasets to the Tensor Database](https://docs.activeloop.ai/examples/rag/managed-database/migrating-datasets-to-the-tensor-database)\n",
    "  - [Deep Memory](https://docs.activeloop.ai/examples/rag/deep-memory)\n",
    "    - [How it Works](https://docs.activeloop.ai/examples/rag/deep-memory/how-it-works)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9134e12-66df-4b48-915d-a3b9f5e1c550",
   "metadata": {},
   "source": [
    "## RAG (LangChain Integration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68283fa7-7c37-4fad-98da-d8583bba93d1",
   "metadata": {},
   "source": [
    "### Use Deep Lake as a Vector Store in LangChain\n",
    "*Deep Lake can be used as a VectorStore in LangChain for building Apps that require filtering and vector search. In this tutorial we will show how to create a Deep Lake Vector Store in LangChain and use it to build a Q&A App about the Twitter OSS recommendation algorithm.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91008277-dc8a-4f46-90af-a7dec91006d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain deeplake openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f0bd42-f7c1-4e76-a8bf-c56f63cf1afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain-deeplake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9f777-57cc-41a1-83e5-465c68380295",
   "metadata": {},
   "source": [
    "#### Downloading and Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a9a252-e5fd-4d80-ac8d-9aaf0947bf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavel\\projects\\ai-llm-deeplake\\.venv\\Lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (4.1.14) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain.vectorstores import DeepLake\n",
    "from langchain_community.vectorstores import DeepLake\n",
    "# from langchain_deeplake.vectorstores import DeeplakeVectorStore\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override = True)\n",
    "open_api_key = os.getenv('OPENAI_API_KEY')\n",
    "activeloop_token = os.getenv('ACTIVELOOP_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1da134b-9b60-4b8c-b4c0-b4f55ccc79b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_GPT = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdd24cbd-4a89-4b11-b4d9-3d0aa5ccef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the Twitter OSS recommendation algorithm\n",
    "\n",
    "# !git clone https://github.com/twitter/the-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01b4dfcf-ff4d-447e-86ee-35e40b29ccfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_prediction_tensor_writer.py\n",
      "batch_prediction_writer.py\n",
      "data_record_tensor_writer.py\n",
      "full_dense.py\n",
      "full_sparse.py\n",
      "isotonic.py\n",
      "layer.py\n",
      "mdl.py\n",
      "partition.py\n",
      "percentile_discretizer.py\n",
      "sequential.py\n",
      "sparse_max_norm.py\n",
      "stitch.py\n",
      "__init__.py\n"
     ]
    }
   ],
   "source": [
    "# Load all the files from the repo into a list\n",
    "\n",
    "repo_path = '/the-algorithm'\n",
    "# repo_path = './the-algorithm'\n",
    "repo_path = './the-algorithm/twml/twml/layers'\n",
    "\n",
    "docs = []\n",
    "for dirpath, dirnames, filenames in os.walk(repo_path):\n",
    "    for file in filenames:\n",
    "        try: \n",
    "            loader = TextLoader(os.path.join(dirpath, file), encoding='utf-8')\n",
    "            docs.extend(loader.load_and_split())\n",
    "            print(file)  # TODO: COMMENT\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faf1413d-2ebb-444e-852a-600af2ab5bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "print(type(docs))\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aadc7e83-e8b4-4829-b640-88571bd7bcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1684, which is longer than the specified 1000\n",
      "Created a chunk of size 1760, which is longer than the specified 1000\n",
      "Created a chunk of size 1157, which is longer than the specified 1000\n",
      "Created a chunk of size 2504, which is longer than the specified 1000\n",
      "Created a chunk of size 1427, which is longer than the specified 1000\n",
      "Created a chunk of size 1438, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "# [Note on chunking text files]\n",
    "# - Text files are typically split into chunks before creating embeddings.\n",
    "# - In general, more chunks increases the relevancy of data that is fed into the language model,\n",
    "#   since granular data can be selected with higher precision.\n",
    "# - However, since an embedding will be created for each chunk, more chunks increase the computational complexity.\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34a6d6bf-d528-47f1-b164-b21c6401a1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "print(type(texts))\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44553564-3a92-4047-8203-5df551b5fe56",
   "metadata": {},
   "source": [
    "**Chunks in the above context should not be confused with Deep Lake chunks!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb75b11-b846-4c8c-8872-5299186d05cb",
   "metadata": {},
   "source": [
    "#### Creating the Deep Lake Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7e0adb2-3378-4088-a6af-13b451cf2e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = 'hub://<org-id>/twitter_algorithm'\n",
    "dataset_path = 'hub://pavelkloscz/twitter_algorithm_twml'  # [twml] subdirectory of this github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e421655-106b-44fa-b05e-08cb61b8afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify an OpenAI algorithm for creating the embeddings, and create the VectorStore.\n",
    "# This process creates an embedding for each element in the texts lists and stores it in Deep Lake format at the specified path\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dce7b2af-7b43-4102-8b5d-058078cf19ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Deep Lake dataset has been successfully created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 86 embeddings in 1 batches of size 86:: 100%|███████████████████████████████████████████████████████████| 1/1 [00:10<00:00, 10.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://pavelkloscz/twitter_algorithm_twml', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      "   text       text      (86, 1)      str     None   \n",
      " metadata     json      (86, 1)      str     None   \n",
      " embedding  embedding  (86, 1536)  float32   None   \n",
      "    id        text      (86, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "db = DeepLake.from_documents(texts, embeddings, dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcee454-b29a-4a25-b917-cce6d0ad8335",
   "metadata": {},
   "source": [
    "**Deep Lake Vector Store has 4 tensors including the text, embedding, ids, and  metadata.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76ebca3-5921-427d-9b7c-e2060b06f710",
   "metadata": {},
   "source": [
    "#### Use the Vector Store in a Q&A App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8f9eee2-9487-4407-bdb8-dcea4dcf65e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavel\\AppData\\Local\\Temp\\ipykernel_27584\\547977038.py:5: LangChainDeprecationWarning: This class is deprecated and will be removed in a future version. You can swap to using the `DeeplakeVectorStore` implementation in `langchain-deeplake`. Please do not submit further PRs to this class.See <https://github.com/activeloopai/langchain-deeplake>\n",
      "  db = DeepLake(dataset_path=dataset_path, read_only=True, embedding=embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://pavelkloscz/twitter_algorithm_twml already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "# Use the VectorStore in Q&A app, where the embeddings will be used to filter relevant documents (texts)\n",
    "#   that are fed into an LLM in order to answer a question.\n",
    "# If we were on another machine, we would load the existing Vector Store without recalculating the embeddings.\n",
    "\n",
    "db = DeepLake(dataset_path=dataset_path, read_only=True, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68152d6c-7730-4c3d-9a3f-9aec6a0535af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever object and specify the search parameters\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "retriever.search_kwargs['distance_metric'] = 'cos'\n",
    "retriever.search_kwargs['k'] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8ba2325-02d1-412f-86cb-25f6b36edcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RetrievalQA chain in LangChain and run it\n",
    "\n",
    "# model = ChatOpenAI(model='gpt-4') # 'gpt-3.5-turbo',\n",
    "model = ChatOpenAI(model=MODEL_GPT) # 'gpt-3.5-turbo',\n",
    "qa = RetrievalQA.from_llm(model, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6ef47bd-4580-48d0-b384-e4f624d6ccf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What programming language is most of the Batch Prediction written in?',\n",
       " 'result': 'Most of the Batch Prediction code is written in Python.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qa.run('What programming language is most of the SimClusters written in?')\n",
    "# qa.invoke('What programming language is most of the SimClusters written in?')\n",
    "qa.invoke('What programming language is most of the Batch Prediction written in?')  ## batch_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a845fbf-03da-4b37-b557-7a73c6b802bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What programming language is most of the TWML written in?',\n",
       " 'result': 'Most of the TWML is written in Python, as indicated by the use of Python syntax and libraries such as TensorFlow in the provided contexts.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.invoke('What programming language is most of the TWML written in?')  ## twml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcdae6c-f66b-48ee-b8c7-c261d0187540",
   "metadata": {},
   "source": [
    "**We can tune k in the retriever depending on whether the prompt exceeds the model's token limit.**<br>\n",
    "**Higher k increases the accuracy by including more data in the prompt.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36724224-be33-4cc5-97f0-a1659d20715c",
   "metadata": {},
   "source": [
    "#### Adding data to to an existing Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f171a3d3-b1fe-49cc-8bfe-30f5527bec5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://pavelkloscz/twitter_algorithm_twml already exists, loading from the storage\n"
     ]
    }
   ],
   "source": [
    "# Data can be added to an existing Vector Store by loading it using its path and adding documents or texts\n",
    "\n",
    "db = DeepLake(dataset_path=dataset_path, embedding=embeddings)\n",
    "\n",
    "# Don't run this here in order to avoid data duplication\n",
    "# db.add_documents(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f5c100-a373-4ba3-8baa-6212f7eb3f66",
   "metadata": {},
   "source": [
    "#### Adding Hybrid Search to the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99f77a76-d275-400e-9e0e-168eaa3af7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since embeddings search can be computationally expensive, you can simplify the search by filtering out data using an\n",
    "#   explicit search on top of the embeddings search. Suppose we want to answer to a question related to the trust and safety models.\n",
    "# We can filter the filenames (source) in the metadata using a custom function that is added to the retriever\n",
    "def filter(deeplake_sample):\n",
    "    return 'trust_and_safety_models' in deeplake_sample['metadata'].data()['value']['source']\n",
    "\n",
    "retriever.search_kwargs['filter'] = filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcef1f47-d0f2-4c4f-9435-3c8bfda82ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 86/86 [00:00<00:00, 145.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What do the trust and safety models do?',\n",
       " 'result': 'Trust and safety models are designed to protect users and maintain a safe environment on platforms, typically in the context of online services and communities. They help identify, prevent, and address harmful behaviors such as harassment, abuse, misinformation, and fraud. These models often include the use of algorithms and human moderation to monitor user interactions, enforce community guidelines, and ensure compliance with legal and ethical standards. Ultimately, their goal is to create a secure and positive experience for all users.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = RetrievalQA.from_llm(model, retriever=retriever)\n",
    "\n",
    "# qa.run(\"What do the trust and safety models do?\")\n",
    "qa.invoke(\"What do the trust and safety models do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d78425ca-8f66-499a-b754-f02081b79967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters can also be specified as a dictionary.\n",
    "# For example, if the metadata tensor had a key year, we can filter based on that key using\n",
    "\n",
    "# retriever.search_kwargs['filter'] = {\"metadata\": {\"year\": 2020}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aacf36-8eb4-406d-b144-e300775799fd",
   "metadata": {},
   "source": [
    "#### Using Deep Lake in Applications that Require Concurrency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ba85b27-6155-443f-bb4d-32fb47e1fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For applications that require writing of data concurrently, users should set up a lock system to queue\n",
    "#   the write operations and prevent multiple clients from writing to the Deep Lake Vector Store at the same time.\n",
    "# This can be done with a few lines of code in the example below\n",
    "\n",
    "# [Concurrency Using Zookeeper Locks]\n",
    "# - https://docs.activeloop.ai/technical-details/best-practices/concurrent-writes/concurrency-using-zookeeper-locks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb0f40-a15e-4fff-a7e0-6da3e3284ad5",
   "metadata": {},
   "source": [
    "#### Accessing the Low Level Deep Lake API (Advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f61dfc9-f9d9-4e84-9035-4f6176d1123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using a Deep Lake Vector Store in LangChain, the underlying Vector Store and its low-level Deep Lake dataset can be accessed via\n",
    "\n",
    "# LangChain Vector Store\n",
    "db = DeepLake(dataset_path=dataset_path)\n",
    "\n",
    "# Deep Lake Vector Store object\n",
    "ds = db.vectorstore\n",
    "\n",
    "# Deep Lake Dataset object\n",
    "ds = db.vectorstore.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aca2206-4ad0-4258-9111-285e94a7dd32",
   "metadata": {},
   "source": [
    "#### SelfQueryRetriever with Deep Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1ca60-fa10-4df7-947d-443e97eb6342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Lake supports the SelfQueryRetriever implementation in LangChain, which translates a user prompt into a metadata filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3667c9-f71c-4086-bafe-6821c31ede0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section of the tutorial requires installation of additional packages\n",
    "\n",
    "# !pip install \"deeplake[enterprise]\" lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ef6d18-a1f1-445e-b7e4-546c6b174e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's create a Deep Lake Vector Store with relevant data using the documents below\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n",
    "        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n",
    "        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n",
    "        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n",
    "        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Toys come alive and have a blast doing so\",\n",
    "        metadata={\"year\": 1995, \"genre\": \"animated\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",\n",
    "        metadata={\n",
    "            \"year\": 1979,\n",
    "            \"rating\": 9.9,\n",
    "            \"director\": \"Andrei Tarkovsky\",\n",
    "            \"genre\": \"science fiction\",\n",
    "            \"rating\": 9.9,\n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd08045-39e8-4741-93bb-ddb8616206c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this feature uses Deep Lake's Tensor Query Language under the hood, the Vector Store must be stored in or connected to Deep Lake,\n",
    "#   which requires registration with Activeloop\n",
    "\n",
    "org_id = <YOUR_ORG_ID> #By default, your username is an org_id\n",
    "dataset_path = f\"hub://{org_id}/self_query\"\n",
    "\n",
    "vectorstore = DeepLake.from_documents(\n",
    "    docs, embeddings, dataset_path = dataset_path, overwrite = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6225dd8-8ce3-4eba-99c6-8603626ae012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our retriever by providing information about the metadata fields that\n",
    "#   our documents support and a short description of the document contents\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The genre of the movie\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"Brief summary of a movie\"\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm, vectorstore, document_content_description, metadata_field_info, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24127e91-196c-4935-b5c6-5ba959568071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our retriever\n",
    "\n",
    "# This example only specifies a relevant query\n",
    "retriever.get_relevant_documents(\"What are some movies about dinosaurs\")\n",
    "\n",
    "# [OUTPUT]\n",
    "# [Document(page_content='A bunch of scientists bring back dinosaurs and mayhem breaks loose', metadata={'year': 1993, 'rating': 7.7, 'genre': 'science fiction'}),\n",
    "#  Document(page_content='Toys come alive and have a blast doing so', metadata={'year': 1995, 'genre': 'animated'}),\n",
    "#  Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'year': 1979, 'rating': 9.9, 'director': 'Andrei Tarkovsky', 'genre': 'science fiction'}),\n",
    "#  Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'year': 2006, 'director': 'Satoshi Kon', 'rating': 8.6})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63001644-ae58-4df9-918f-032f7735a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a query to find movies that are above a certain ranking\n",
    "\n",
    "# This example only specifies a filter\n",
    "retriever.get_relevant_documents(\"I want to watch a movie rated higher than 8.5\")\n",
    "\n",
    "# [OUTPUT]\n",
    "# [Document(page_content='A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea', metadata={'year': 2006, 'director': 'Satoshi Kon', 'rating': 8.6}),\n",
    "#  Document(page_content='Three men walk into the Zone, three men walk out of the Zone', metadata={'year': 1979, 'rating': 9.9, 'director': 'Andrei Tarkovsky', 'genre': 'science fiction'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69acbcf4-e080-43d5-86d5-487e58f4a750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets\n",
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "007323c9-4013-4eda-a84f-f9668951a2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavel\\projects\\ai-llm-deeplake\\.venv\\Lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (4.1.14) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from deeplake import VectorStore\n",
    "# from deeplake.core.vectorstore.deeplake_vectorstore import VectorStore\n",
    "# from deeplake.core.vectorstore import VectorStore\n",
    "import os\n",
    "import getpass\n",
    "import datasets\n",
    "import openai\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override = True)\n",
    "open_api_key = os.getenv('OPENAI_API_KEY')\n",
    "activeloop_token = os.getenv('ACTIVELOOP_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19538e8b-0138-46f6-a171-03541a3f0811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105001734a9a44728f5498fd1a4c4342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0617943d8140ad8c05fa4188fb3748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5183 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download the dataset locally\n",
    "\n",
    "# corpus = datasets.load_dataset(\"scifact\", \"corpus\")\n",
    "corpus = datasets.load_dataset(\"scifact\", \"corpus\", trust_remote_code=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
