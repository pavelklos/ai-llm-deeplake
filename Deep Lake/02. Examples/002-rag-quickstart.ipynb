{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93495a2e-3e04-4cb8-81d7-a00f9dc0b4f4",
   "metadata": {},
   "source": [
    "# EXAMPLES (RAG)\n",
    "- [RAG](https://docs.activeloop.ai/examples/rag)\n",
    "  - [**RAG Quickstart**](https://docs.activeloop.ai/examples/rag/quickstart)\n",
    "  - [RAG Tutorials](https://docs.activeloop.ai/examples/rag/tutorials)\n",
    "    - [Vector Store Basics](https://docs.activeloop.ai/examples/rag/tutorials/vector-store-basics)\n",
    "    - [Vector Search Options](https://docs.activeloop.ai/examples/rag/tutorials/vector-search-options)\n",
    "      - [LangChain API](https://docs.activeloop.ai/examples/rag/tutorials/vector-search-options/langchain-api)\n",
    "      - [Deep Lake Vector Store API](https://docs.activeloop.ai/examples/rag/tutorials/vector-search-options/vector-store-api)\n",
    "      - [Managed Database REST API](https://docs.activeloop.ai/examples/rag/tutorials/vector-search-options/rest-api)\n",
    "    - [Customizing Your Vector Store](https://docs.activeloop.ai/examples/rag/tutorials/step-4-customizing-vector-stores)\n",
    "    - [Image Similarity Search](https://docs.activeloop.ai/examples/rag/tutorials/image-similarity-search)\n",
    "    - [Improving Search Accuracy using Deep Memory](https://docs.activeloop.ai/examples/rag/tutorials/deepmemory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9134e12-66df-4b48-915d-a3b9f5e1c550",
   "metadata": {},
   "source": [
    "## RAG Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e36525b-23e7-4fce-b940-737862635009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Deep Lake\n",
    "\n",
    "# !pip3 install deeplake\n",
    "# !pip3 install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f1ee05-25b0-41ef-b444-192b01cc04ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Your First Vector Store\n",
    "\n",
    "# [paul_graham_essay.txt] file (73 kB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7187d032-b903-433d-ba2e-6f654ab5e4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavel\\projects\\ai-llm-deeplake\\.venv\\Lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (4.1.13) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import the required modules and set the OpenAI environmental variables for embeddings\n",
    "# os.environ['OPENAI_API_KEY'] = <OPENAI_API_KEY>\n",
    "\n",
    "from deeplake.core.vectorstore import VectorStore\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override = True)\n",
    "open_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8550439d-2aac-4283-b00a-b64273a5f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and chunk the essay text based on a constant number of characters\n",
    "\n",
    "source_text = 'paul_graham_essay.txt'\n",
    "# source_text = '../../data/paul_graham/paul_graham_essay.txt'\n",
    "vector_store_path = 'pg_essay_deeplake'\n",
    "\n",
    "with open(source_text, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "CHUNK_SIZE = 1000\n",
    "chunked_text = [text[i:i+CHUNK_SIZE] for i in range(0,len(text), CHUNK_SIZE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1782c2d-8ad1-4819-b712-b42514b36723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "print(type(chunked_text))\n",
    "print(len(chunked_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0cf2518-2c5b-4147-a4c4-84f7e372c187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhat I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines â€” CPU, disk drives, printer, card reader â€” sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b16a949-b4be-4561-bc9a-4d3f3547d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an embedding function using OpenAI\n",
    "\n",
    "def embedding_function(texts, model=\"text-embedding-ada-002\"):\n",
    "   \n",
    "   if isinstance(texts, str):\n",
    "       texts = [texts]\n",
    "\n",
    "   texts = [t.replace(\"\\n\", \" \") for t in texts]\n",
    "   \n",
    "   return [data.embedding for data in openai.embeddings.create(input = texts, model=model).data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deb97afa-5938-4ccd-8ce7-e5e0a4d89daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 76 embeddings in 1 batches of size 76:: 100%|███████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='pg_essay_deeplake', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      "   text       text      (76, 1)      str     None   \n",
      " metadata     json      (76, 1)      str     None   \n",
      " embedding  embedding  (76, 1536)  float32   None   \n",
      "    id        text      (76, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the Deep Lake Vector Store and populate it with data\n",
    "\n",
    "vector_store = VectorStore(\n",
    "    path = vector_store_path,\n",
    ")\n",
    "\n",
    "vector_store.add(text = chunked_text, \n",
    "                 embedding_function = embedding_function, \n",
    "                 embedding_data = chunked_text, \n",
    "                 metadata = [{\"source\": source_text}]*len(chunked_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9e979cd-46b6-47f7-8c96-3444a5d211e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='pg_essay_deeplake', tensors=['text', 'metadata', 'embedding', 'id'])\n",
      "\n",
      "  tensor      htype      shape      dtype  compression\n",
      "  -------    -------    -------    -------  ------- \n",
      "   text       text      (76, 1)      str     None   \n",
      " metadata     json      (76, 1)      str     None   \n",
      " embedding  embedding  (76, 1536)  float32   None   \n",
      "    id        text      (76, 1)      str     None   \n"
     ]
    }
   ],
   "source": [
    "# The Vector Store's data structure can be summarized using\n",
    "\n",
    "vector_store.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "691d4711-f94d-4c58-8867-274e6167478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create a vector store using pre-compute embeddings instead of\n",
    "#   the embedding_data and embedding_function, you may run\n",
    "\n",
    "# vector_store.add(text = chunked_text, \n",
    "#                  embedding = <list_of_embeddings>, \n",
    "#                  metadata = [{\"source\": source_text}]*len(chunked_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c43ce1f2-36c4-4fe8-a159-d8e80078b69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing Vector Search\n",
    "\n",
    "prompt = \"What are the first programs he tried writing?\"\n",
    "\n",
    "search_results = vector_store.search(embedding_data=prompt, embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "083ce60f-ee16-403b-b06f-28f2c40c5ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhat I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines â€” CPU, disk drives, printer, card reader â€” sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The search_results is a dictionary with keys for the\n",
    "#   text, score, id, and metadata, with data ordered by score.\n",
    "# If we examine the first returned text using search_results['text'][0],\n",
    "#   it appears to contain the answer to the prompt.\n",
    "\n",
    "search_results['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "235428a2-9e3f-468f-840d-36a6c206fba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "What I Worked On\n",
      "\n",
      "February 2021\n",
      "\n",
      "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
      "\n",
      "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines â€” CPU, disk drives, printer, card reader â€” sitting up on a raised floor under bright fluorescent lights.\n",
      "\n",
      "The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them \n"
     ]
    }
   ],
   "source": [
    "print(search_results['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87b41d68-d503-4e07-8c35-e4a548aa3409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing your Vector Store\n",
    "# - https://app.activeloop.ai/activeloop/twitter-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7965c12-53f3-4b2f-a17a-d3e39ec6af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication\n",
    "# - https://app.activeloop.ai/register/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7af2ab36-60f9-4890-8e57-e8cc1cbdbb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Vector Stores in the Deep Lake Managed Tensor Database\n",
    "\n",
    "# vector_store = VectorStore(\n",
    "#     path = vector_store_path,\n",
    "#     runtime = {\"tensor_db\": True}\n",
    "# )\n",
    "\n",
    "# vector_store.add(text = chunked_text, \n",
    "#                  embedding_function = embedding_function, \n",
    "#                  embedding_data = chunked_text, \n",
    "#                  metadata = [{\"source\": source_text}]*len(chunked_text))\n",
    "                 \n",
    "# search_results = vector_store.search(embedding_data = prompt, \n",
    "#                                      embedding_function = embedding_function)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
